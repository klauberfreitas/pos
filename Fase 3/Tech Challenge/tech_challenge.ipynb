{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tech Challenge Fase 3\n",
    "\n",
    "# Aluno\n",
    "Klauber Lage - RM358972\n",
    "\n",
    "Link v√≠deo no Youtube: https://www.youtube.com/watch?v=sYGzyT-TW40\n",
    "\n",
    "Link reposit√≥rio no GitHub: https://github.com/klauberfreitas/pos/tree/main/Fase%203/Tech%20Challenge\n",
    "\n",
    "# O Problema\n",
    "O desafio consiste em projetar, testar e implementar um modelo fine tuned usando foudation model. O modelo deve ser treinado usando o dataset da Amazon, AmazonTitles-1.3MM. Este modelo dever√° tamb√©m ser capaz de responder perguntas por meio de integra√ß√£o RAG (Retrieve-and-Generate), usando este mesmo dataset, citando as fontes.\n",
    "\n",
    "# Tarefas\n",
    "REQUISITOS DO PROJETO\n",
    "\n",
    "- Treinar um modelo atrav√©s de Fine Tuning usando o dataset da Amazon\n",
    "- Usar o modelo treinado para a integra√ß√£o via RAG\n",
    "- Usar o dataset da Amazon para o RAG\n",
    "- Fazer perguntas e obter respostas, inclu√≠ndo as fontes\n",
    "\n",
    "\n",
    "### Refer√™ncia:\n",
    "[Documento PDF do Desafio](3IADT - Fase 3 - Tech Challenge.pdf)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relat√≥rio "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Instala√ß√£o de Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.49.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.10.0)\n",
      "Requirement already satisfied: torch in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: rank-bm25 in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.2.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.29.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kake\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (3.9.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\kake\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from yarl<2.0,>=1.0->aiohttp->datasets) (0.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy scikit-learn transformers sentence-transformers faiss-cpu torch rank-bm25 datasets huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Carregamento e Limpeza do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carregar dados em chunks\n",
    "def load_data(path, chunksize=10000):\n",
    "    chunks = pd.read_json(path, lines=True, chunksize=chunksize)\n",
    "    return pd.concat(chunks)\n",
    "\n",
    "df_raw = load_data('./data/trn.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorando o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>target_ind</th>\n",
       "      <th>target_rel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000031909</td>\n",
       "      <td>Girls Ballet Tutu Neon Pink</td>\n",
       "      <td>High quality 3 layer ballet tutu. 12 inches in...</td>\n",
       "      <td>[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 2...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000032034</td>\n",
       "      <td>Adult Ballet Tutu Yellow</td>\n",
       "      <td></td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 16, 33, 36, 37,...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000913154</td>\n",
       "      <td>The Way Things Work: An Illustrated Encycloped...</td>\n",
       "      <td></td>\n",
       "      <td>[116, 117, 118, 119, 120, 121, 122]</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001360000</td>\n",
       "      <td>Mog's Kittens</td>\n",
       "      <td>Judith Kerr&amp;#8217;s best&amp;#8211;selling adventu...</td>\n",
       "      <td>[146, 147, 148, 149, 495]</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001381245</td>\n",
       "      <td>Misty of Chincoteague</td>\n",
       "      <td></td>\n",
       "      <td>[151]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0001371045</td>\n",
       "      <td>Hilda Boswell's treasury of children's stories...</td>\n",
       "      <td></td>\n",
       "      <td>[150]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0000230022</td>\n",
       "      <td>The Simple Truths of Service: Inspired by John...</td>\n",
       "      <td></td>\n",
       "      <td>[184, 185, 186, 187, 188, 189, 190, 191, 192, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0000031895</td>\n",
       "      <td>Girls Ballet Tutu Neon Blue</td>\n",
       "      <td>Dance tutu for girls ages 2-8 years. Perfect f...</td>\n",
       "      <td>[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 2...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0000174076</td>\n",
       "      <td>Evaluating Research in Academic Journals - A P...</td>\n",
       "      <td></td>\n",
       "      <td>[106, 208, 209, 210, 211, 212, 213, 214, 215, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0001713086</td>\n",
       "      <td>Dr. Seuss ABC (Dr.Seuss Classic Collection) (S...</td>\n",
       "      <td></td>\n",
       "      <td>[260, 261, 262, 263, 264, 265, 266, 267]</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0001361155</td>\n",
       "      <td>Noddy Story Book Treasury</td>\n",
       "      <td></td>\n",
       "      <td>[268, 269, 270, 271, 272, 273, 274, 275, 276, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0001472933</td>\n",
       "      <td>The Book of Daniel</td>\n",
       "      <td></td>\n",
       "      <td>[181, 182, 305, 306, 307, 308, 309, 310, 311, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>000100039X</td>\n",
       "      <td>The Prophet</td>\n",
       "      <td>In a distant, timeless place, a mysterious pro...</td>\n",
       "      <td>[329, 330, 331, 332, 333, 334, 335, 336, 337, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0001473905</td>\n",
       "      <td>Rightly Dividing the Word</td>\n",
       "      <td>--This text refers to thePaperbackedition.</td>\n",
       "      <td>[181, 182, 307, 380, 381]</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>000047715X</td>\n",
       "      <td>Mksap 16 Audio Companion: Medical Knowledge Se...</td>\n",
       "      <td></td>\n",
       "      <td>[382, 383, 384, 385, 386, 387, 388, 389, 390, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0001516035</td>\n",
       "      <td>Worship with Don Moen [VHS]</td>\n",
       "      <td>Worship with Don Moen [VHS]</td>\n",
       "      <td>[422]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0001837397</td>\n",
       "      <td>Autumn Story Brambly Hedge</td>\n",
       "      <td>\"!the most research-crammed fantasy ever set b...</td>\n",
       "      <td>[423, 424, 425, 427, 430, 438, 439, 440]</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0000791156</td>\n",
       "      <td>Spirit Led-Moving By Grace In The Holy Spirit'...</td>\n",
       "      <td>You can flow effortlessly and powerfully in th...</td>\n",
       "      <td>[441, 442, 443, 444, 445, 446]</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0001714384</td>\n",
       "      <td>The Very Bad Bunny (Beginner Series)</td>\n",
       "      <td>By Marilyn Sadler, Illustrated by Roger Bollen</td>\n",
       "      <td>[483, 484, 485, 486, 487]</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0001950584</td>\n",
       "      <td>The Bayeux Tapestry: The Norman Conquest 1066</td>\n",
       "      <td></td>\n",
       "      <td>[488, 489, 490, 491, 492, 493]</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           uid                                              title  \\\n",
       "0   0000031909                        Girls Ballet Tutu Neon Pink   \n",
       "1   0000032034                           Adult Ballet Tutu Yellow   \n",
       "2   0000913154  The Way Things Work: An Illustrated Encycloped...   \n",
       "3   0001360000                                      Mog's Kittens   \n",
       "4   0001381245                              Misty of Chincoteague   \n",
       "5   0001371045  Hilda Boswell's treasury of children's stories...   \n",
       "6   0000230022  The Simple Truths of Service: Inspired by John...   \n",
       "7   0000031895                        Girls Ballet Tutu Neon Blue   \n",
       "8   0000174076  Evaluating Research in Academic Journals - A P...   \n",
       "9   0001713086  Dr. Seuss ABC (Dr.Seuss Classic Collection) (S...   \n",
       "10  0001361155                          Noddy Story Book Treasury   \n",
       "11  0001472933                                 The Book of Daniel   \n",
       "12  000100039X                                        The Prophet   \n",
       "13  0001473905                          Rightly Dividing the Word   \n",
       "14  000047715X  Mksap 16 Audio Companion: Medical Knowledge Se...   \n",
       "15  0001516035                        Worship with Don Moen [VHS]   \n",
       "16  0001837397                         Autumn Story Brambly Hedge   \n",
       "17  0000791156  Spirit Led-Moving By Grace In The Holy Spirit'...   \n",
       "18  0001714384               The Very Bad Bunny (Beginner Series)   \n",
       "19  0001950584      The Bayeux Tapestry: The Norman Conquest 1066   \n",
       "\n",
       "                                              content  \\\n",
       "0   High quality 3 layer ballet tutu. 12 inches in...   \n",
       "1                                                       \n",
       "2                                                       \n",
       "3   Judith Kerr&#8217;s best&#8211;selling adventu...   \n",
       "4                                                       \n",
       "5                                                       \n",
       "6                                                       \n",
       "7   Dance tutu for girls ages 2-8 years. Perfect f...   \n",
       "8                                                       \n",
       "9                                                       \n",
       "10                                                      \n",
       "11                                                      \n",
       "12  In a distant, timeless place, a mysterious pro...   \n",
       "13         --This text refers to thePaperbackedition.   \n",
       "14                                                      \n",
       "15                        Worship with Don Moen [VHS]   \n",
       "16  \"!the most research-crammed fantasy ever set b...   \n",
       "17  You can flow effortlessly and powerfully in th...   \n",
       "18     By Marilyn Sadler, Illustrated by Roger Bollen   \n",
       "19                                                      \n",
       "\n",
       "                                           target_ind  \\\n",
       "0   [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 2...   \n",
       "1   [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 16, 33, 36, 37,...   \n",
       "2                 [116, 117, 118, 119, 120, 121, 122]   \n",
       "3                           [146, 147, 148, 149, 495]   \n",
       "4                                               [151]   \n",
       "5                                               [150]   \n",
       "6   [184, 185, 186, 187, 188, 189, 190, 191, 192, ...   \n",
       "7   [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 2...   \n",
       "8   [106, 208, 209, 210, 211, 212, 213, 214, 215, ...   \n",
       "9            [260, 261, 262, 263, 264, 265, 266, 267]   \n",
       "10  [268, 269, 270, 271, 272, 273, 274, 275, 276, ...   \n",
       "11  [181, 182, 305, 306, 307, 308, 309, 310, 311, ...   \n",
       "12  [329, 330, 331, 332, 333, 334, 335, 336, 337, ...   \n",
       "13                          [181, 182, 307, 380, 381]   \n",
       "14  [382, 383, 384, 385, 386, 387, 388, 389, 390, ...   \n",
       "15                                              [422]   \n",
       "16           [423, 424, 425, 427, 430, 438, 439, 440]   \n",
       "17                     [441, 442, 443, 444, 445, 446]   \n",
       "18                          [483, 484, 485, 486, 487]   \n",
       "19                     [488, 489, 490, 491, 492, 493]   \n",
       "\n",
       "                                           target_rel  \n",
       "0   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...  \n",
       "1   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...  \n",
       "2                 [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]  \n",
       "3                           [1.0, 1.0, 1.0, 1.0, 1.0]  \n",
       "4                                               [1.0]  \n",
       "5                                               [1.0]  \n",
       "6   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...  \n",
       "7   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...  \n",
       "8   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...  \n",
       "9            [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]  \n",
       "10  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...  \n",
       "11  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...  \n",
       "12  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...  \n",
       "13                          [1.0, 1.0, 1.0, 1.0, 1.0]  \n",
       "14  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...  \n",
       "15                                              [1.0]  \n",
       "16           [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]  \n",
       "17                     [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]  \n",
       "18                          [1.0, 1.0, 1.0, 1.0, 1.0]  \n",
       "19                     [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos perceber a quantidade de nulos. Portanto vamos tratar o dataset, limpando estes valores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpeza do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados ap√≥s limpeza: restaram 1367131 de 2248619 registros\n"
     ]
    }
   ],
   "source": [
    "# Fun√ß√£o para limpar o dataset\n",
    "def clean_dataset(df):\n",
    "    # Remove registros com titles nulos ou vazios\n",
    "    df = df[df['title'].notna() & (df['title'].str.strip() != '')]\n",
    "    \n",
    "    # Remove registros com contents nulos ou vazios\n",
    "    df = df[df['content'].notna() & (df['content'].str.strip() != '')]\n",
    "    \n",
    "    # Remove duplicados, conforme podemos ver acima\n",
    "    cols_to_check = ['title']\n",
    "\n",
    "    cols_to_check.append('content')\n",
    "    \n",
    "    df = df.drop_duplicates(subset=cols_to_check)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Chama a fun√ß√£o que limpa o dataset\n",
    "df = clean_dataset(df_raw)\n",
    "\n",
    "print(f\"Dados ap√≥s limpeza: restaram {len(df)} de {len(df_raw)} registros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prepara√ß√£o dos Dados para Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1093704/1093704 [02:37<00:00, 6938.72 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 273427/273427 [00:38<00:00, 7126.69 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Divis√£o em treino e teste\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convers√£o para o formato do Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "# Fun√ß√£o para tokenizar e formatar os dados de entrada\n",
    "def preprocess(examples):\n",
    "    questions = examples['content']\n",
    "    contexts = examples['title']\n",
    "    targets = examples['title']\n",
    "    \n",
    "    # Cria as entradas para o tokenizer combinando o t√≠tulo com o content\n",
    "    inputs = [f\"question: {question} context: {context}\" for question, context in zip(questions, contexts)]\n",
    "    \n",
    "    # Tokenizar e formatar as entradas do modelo\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,  # Perguntas e Contextos\n",
    "        max_length=512,  # Define o comprimento m√°ximo das sequ√™ncias de tokens\n",
    "        truncation=True,  # Trunca as sequ√™ncias que excedem o comprimento m√°ximo\n",
    "        padding=\"max_length\"  # Adiciona padding para que todas as sequ√™ncias tenham o mesmo comprimento\n",
    "    )\n",
    "    \n",
    "    # Tokenizar targets\n",
    "    labels = tokenizer(\n",
    "        targets,  # T√≠tulos\n",
    "        max_length=128,  # Define o comprimento m√°ximo das sequ√™ncias de tokens\n",
    "        truncation=True,  # Trunca as sequ√™ncias de target que excedem o comprimento m√°ximo\n",
    "        padding=\"max_length\"  # Adiciona padding para que todas as sequ√™ncias de target tenham o mesmo comprimento\n",
    "    )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Aplicar pr√©-processamento\n",
    "tokenized_train = train_dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "tokenized_test = test_dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=test_dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Configura√ß√µes do treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vari√°veis que controlam o comportamento do Fine Tuning e do RAG\n",
    "\n",
    "# N√∫mero de √©pocas do treinamento\n",
    "EPOCHS=5\n",
    "\n",
    "# Definido para o que a minha VRAM aguenta\n",
    "TRAIN_BATCH_SIZE=8\n",
    "\n",
    "# Batch efetivo\n",
    "ACCUMULATION_STEPS=2\n",
    "\n",
    "# N√∫mero m√°ximo de steps do fine tuning\n",
    "# MAX_STEPS=(len(train_dataset) // TRAIN_BATCH_SIZE) * EPOCHS # Este seria o valor ideal, por√©m levar√° muito para terminar durante o exemplo\n",
    "MAX_STEPS=20\n",
    "\n",
    "# N√∫mero de documentos do dataset a serem usados. Esou usando apenas 500 amostras devido ao alto consumo de mem√≥ria e CPU, o que estava levando o PC a travar\n",
    "DOCUMENTS=500\n",
    "\n",
    "# Top K de resultados para o retriever\n",
    "K=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Remove o conte√∫do da pasta results para evitar erros de pasta j√° existente\n",
    "try:\n",
    "    shutil.rmtree(f\"./results/checkpoint-{MAX_STEPS}\")\n",
    "except OSError as e:\n",
    "    print(\"Error: %s - %s.\" % (e.filename, e.strerror))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kake\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\kake\\AppData\\Local\\Temp\\ipykernel_16072\\616683413.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 05:41, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20, training_loss=13.435350036621093, metrics={'train_runtime': 364.9294, 'train_samples_per_second': 0.877, 'train_steps_per_second': 0.055, 'total_flos': 219122352783360.0, 'train_loss': 13.435350036621093, 'epoch': 0.6349206349206349})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# Carrega o modelo T5 pr√©-treinado da Google\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "# Configura√ß√£o de treino\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",  # Pasta para salvar os resultados\n",
    "    evaluation_strategy=\"no\",  # Desliga a avalia√ß√£o durante o treino j√° que estou limitando a 500 documentos e para economizar recursos\n",
    "    learning_rate=2e-4,  # Aumenta taxa de aprendizado, o ideal √© entre 1e-4 a 3e-4\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,  # Definido para o que a minha VRAM aguenta\n",
    "    gradient_accumulation_steps=ACCUMULATION_STEPS,  # Batch efetivo de TRAIN_BATCH_SIZE*ACCUMULATION_STEPS=16\n",
    "    weight_decay=0.01,  # Decaimento de peso para regulariza√ß√£o\n",
    "    save_total_limit=1,  # Limita o n√∫mero de checkpoints salvos\n",
    "    num_train_epochs=EPOCHS,  # N√∫mero de √©pocas de treinamento\n",
    "    fp16=True,  # Usa precis√£o mista para reduzir uso de mem√≥ria (meu pc estava travando)\n",
    "    logging_steps=100,  # Frequ√™ncia de logging\n",
    "    max_steps=MAX_STEPS,  # Limita o n√∫mero total de passos\n",
    "    optim=\"adafactor\",  # Optimizer mais leve\n",
    "    gradient_checkpointing=True,  # Reduz uso de VRAM\n",
    ")\n",
    "\n",
    "# Dataset de treinamento e de avalia√ß√£o\n",
    "train_dataset = tokenized_train.select(range(DOCUMENTS))  \n",
    "eval_dataset = tokenized_test.select(range(DOCUMENTS)) \n",
    "\n",
    "# Data Collator para prepara√ß√£o dos dados\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,  # Tokenizador a ser usado\n",
    "    model=model,  # Modelo a ser usado (que √© o flan-t5-base)\n",
    "    padding=True  # Adiciona padding para uniformizar o tamanho das sequ√™ncias (garantindo que todas as entradas tenham o mesmo comprimento) al√©m de acelerar o treinamento\n",
    ")\n",
    "\n",
    "# Cria o Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,  # Modelo a ser treinado (que √© o flan-t5-base)\n",
    "    args=training_args,  # Argumentos de treinamento\n",
    "    train_dataset=train_dataset,  # Conjunto de dados de treinamento\n",
    "    eval_dataset=eval_dataset,  # Conjunto de dados de avalia√ß√£o\n",
    "    data_collator=data_collator,  # Data collator para prepara√ß√£o dos dados\n",
    "    tokenizer=tokenizer,  # Tokenizador a ser usado\n",
    ")\n",
    "\n",
    "# Inicia o treinamento\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Carregamento do modelo treinado e Estruturas de Gera√ß√£o de Respostas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "# Configura o caminho para o modelo fine tuned\n",
    "model_path = f\"./results/checkpoint-{MAX_STEPS}\"\n",
    "\n",
    "# Carrega o modelo e o tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path).to(\"cpu\")\n",
    "\n",
    "documents = (df['title'] + \": \" + df['content']).tolist()\n",
    "\n",
    "# Classe para criar a base do retriever\n",
    "class SemanticRetriever:\n",
    "    def __init__(self, documents, batch_size=256):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.encoder = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2').to(self.device)\n",
    "        self.documents = documents\n",
    "        self.batch_size = batch_size\n",
    "        self.embeddings = self.encode_documents(documents)\n",
    "\n",
    "    #  Calcula e armazena os embeddings dos documentos retornando em um √∫nico tensor\n",
    "    def encode_documents(self, documents):\n",
    "        embeddings = []\n",
    "        for i in range(0, len(documents), self.batch_size):\n",
    "            batch = documents[i:i + self.batch_size]\n",
    "            batch_embeddings = self.encoder.encode(\n",
    "                batch,\n",
    "                convert_to_tensor=True,\n",
    "                normalize_embeddings=True\n",
    "            )\n",
    "            embeddings.append(batch_embeddings)\n",
    "        return torch.cat(embeddings)\n",
    "\n",
    "    # Recupera os documentos mais relevantes para as perguntas que forem feitas\n",
    "    def retrieve(self, query, k=5):\n",
    "        query_emb = self.encoder.encode(\n",
    "            query,\n",
    "            convert_to_tensor=True,\n",
    "            normalize_embeddings=True\n",
    "        )\n",
    "        scores = torch.mm(self.embeddings, query_emb.unsqueeze(0).T)\n",
    "        top_indices = torch.topk(scores.flatten(), k).indices\n",
    "        return [self.documents[i] for i in top_indices]\n",
    "\n",
    "# Fun√ß√£o para gerar as respostas de embeddings\n",
    "def generate_answer(question):\n",
    "    contexts = retriever.retrieve(question, k=K)\n",
    "\n",
    "    # Template de prompt otimizado\n",
    "    context_str = \"\\n\".join([f\"- {context}\" for context in contexts])\n",
    "    instruction = f\"Generate a comprehensive answer using only the following context: Context: {context_str} Question: {question} Answer:\"\n",
    "\n",
    "    # Tokeniza o input, convertendo em tensores\n",
    "    inputs = tokenizer(\n",
    "        instruction,\n",
    "        return_tensors=\"pt\",  # Retorna tensores do PyTorch\n",
    "        max_length=1024,      # Define a sequ√™ncia m√°xima de tokens\n",
    "        truncation=True       # Trunca sequ√™ncias de tokens que excedem o comprimento m√°ximo\n",
    "    ).to(model.device)     \n",
    "\n",
    "    # Gera a resposta com base nos inputs j√° tokenizados\n",
    "    outputs = model.generate(\n",
    "        **inputs,                # Passa os inputs tokenizados para o modelo\n",
    "        max_new_tokens=256,      # Define o n√∫mero m√°ximo de novos tokens a serem gerados\n",
    "        num_beams=7,             # Utiliza beam search para melhorar qualidade da gera√ß√£o\n",
    "        no_repeat_ngram_size=3,  # Evita a repeti√ß√£o de n-gramas\n",
    "        early_stopping=True,     # Interrompe a gera√ß√£o assim que a resposta estiver completa\n",
    "        temperature=0.7          # Controla a aleatoriedade da gera√ß√£o (valores mais baixos tornam a sa√≠da mais determin√≠stica)\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Configura o retriever\n",
    "retriever = SemanticRetriever(documents[:DOCUMENTS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Processamento de Perguntas e Respostas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Fun√ß√£o para carregar as perguntas do arquivo perguntas.json\n",
    "def load_questions_from_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "# Fun√ß√£o para processar as perguntas e respostas\n",
    "def process_questions(file_path):\n",
    "    questions_data = load_questions_from_json(file_path)\n",
    "    \n",
    "    for item in questions_data:\n",
    "        question = item['question']\n",
    "        expected_answer = item['expected']\n",
    "        \n",
    "        resposta = generate_answer(question)\n",
    "        context = retriever.retrieve(question, k=3)\n",
    "        accurate = resposta == expected_answer\n",
    "        \n",
    "        print(f\"Pergunta: {question}\")\n",
    "        print(f\"Resposta gerada: {resposta}\")\n",
    "        print(f\"Resposta esperada: {expected_answer}\")\n",
    "        print(f\"Contexto: {context}\")\n",
    "        print(f\"Resposta correta: {accurate}\")\n",
    "        print(\"-\" * 50)\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kake\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pergunta: Where Jill Barklem was born?\n",
      "Resposta gerada: Epping\n",
      "Resposta esperada: Epping\n",
      "Contexto: [\"Nice for Mice: Jill Barklem was born in Epping in 1951. After an accident when she was thirteen, Jill was unable to take part in PE or games at school and instead developed her talent for drawing and art. On leaving school, she studied illustration at St Martin's in London. Jill is now a full-time illustrator, working on the series of Brambly Hedge books.\", 'Dorothy Rowe\\'s Guide to Life: Dorothy Rowe was born in Australia in 1930, and worked as a teacher and child psychologist before coming to England, where she obtained her PhD at Sheffield University. From 1972 until 1986 she was head of Clinical Psychology. She is now engaged in writing, lecturing and research, and is world-renowned for her work on how we communicate and why we suffer. Her books include \"Wanting Everything\\', \"Beyond Fear\\' and \"Time On Our Side\\'.', 'Animal Wisdom: Jessica Palmer is descended from Dakota woodland Sioux. Although she now lives in the UK she still has strong US contacts. Until now she has concentrated mainly on fiction and has published 10 novels. She regularly gives talks on Native American culture around the country.']\n",
      "Resposta correta: True\n",
      "--------------------------------------------------\n",
      "Pergunta: When was Jill Barklem born?\n",
      "Resposta gerada: 1951\n",
      "Resposta esperada: 1951\n",
      "Contexto: [\"Nice for Mice: Jill Barklem was born in Epping in 1951. After an accident when she was thirteen, Jill was unable to take part in PE or games at school and instead developed her talent for drawing and art. On leaving school, she studied illustration at St Martin's in London. Jill is now a full-time illustrator, working on the series of Brambly Hedge books.\", 'Dorothy Rowe\\'s Guide to Life: Dorothy Rowe was born in Australia in 1930, and worked as a teacher and child psychologist before coming to England, where she obtained her PhD at Sheffield University. From 1972 until 1986 she was head of Clinical Psychology. She is now engaged in writing, lecturing and research, and is world-renowned for her work on how we communicate and why we suffer. Her books include \"Wanting Everything\\', \"Beyond Fear\\' and \"Time On Our Side\\'.', \"The Mystery of Holly Lane (The 5 find-outers): Enid Blyton Enid Blyton died in 1968 but remains one of the best-known and best-loved children's authors. The characters in her stories have been enjoyed for generations and she is consistently voted number one in children's favourite author polls. She has over 600 children's books to her credit, including series such as Malory Towers, St Clare's, The Faraway Tree, The Wishing-Chair and Famous Five.--This text refers to an alternatePaperbackedition.\"]\n",
      "Resposta correta: True\n",
      "--------------------------------------------------\n",
      "Pergunta: Who illustrated The Very Bad Bunny?\n",
      "Resposta gerada: Roger Bollen\n",
      "Resposta esperada: Roger Bollen\n",
      "Contexto: ['The Very Bad Bunny (Beginner Series): By Marilyn Sadler, Illustrated by Roger Bollen', 'I Spy Animals in Art: Selected by Lucy Micklethwait', 'The Illustrated Man (Voyager Classics): ThatThe Illustrated Manhas remained in print since being  published in 1951 is fair testimony to the universal appeal of Ray Bradbury\\'s work. Only his second collection (the first wasDark Carnival, later reworked intoThe October Country), it is a marvelous, if mostly dark, quilt of science fiction, fantasy, and horror. In an ingenious framework to open and close the book, Bradbury presents himself as a nameless narrator who meets the Illustrated Man--a wanderer whose entire body is a living canvas of exotic tattoos. What\\'s even more remarkable, and increasingly disturbing, is that the illustrations are themselves magically alive, and each proceeds to unfold its own story, such as \"The Veldt,\" wherein rowdy children take a game of virtual reality way over the edge. Or \"Kaleidoscope,\" a heartbreaking portrait of stranded astronauts about to reenter our atmosphere--without the benefit of a spaceship. Or \"Zero Hour,\" in which invading aliens have discovered a most logical ally--our own children. Even though most were written in the 1940s and 1950s, these 18 classic stories will be just as chillingly effective 50 years from now.--Stanley Wiater--This text refers to an out of print or unavailable edition of this title.']\n",
      "Resposta correta: True\n",
      "--------------------------------------------------\n",
      "Pergunta: What is perfect for girls ages 2-8 years?\n",
      "Resposta gerada: Ballet Tutu Neon Blue\n",
      "Resposta esperada: Ballet Tutu Neon Blue\n",
      "Contexto: ['Girls Ballet Tutu Neon Blue: Dance tutu for girls ages 2-8 years. Perfect for dance practice, recitals and performances, costumes or just for fun!', 'Girls Ballet Tutu Neon Pink: High quality 3 layer ballet tutu. 12 inches in length', 'Bless My Little Girl: Book by']\n",
      "Resposta correta: True\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "file_path = './perguntas.json'\n",
    "process_questions(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pergunta: Who wrote The Very Bad Bunny?\n",
      "Resposta gerada: Marilyn Sadler\n",
      "Resposta esperada: Marilyn Sadler\n",
      "Contexto: ['The Very Bad Bunny (Beginner Series): By Marilyn Sadler, Illustrated by Roger Bollen', 'Evil Under the Sun: Complete &amp; Unabridged: \"You can\\'t go wrong with this one\" Books \"She springs her secret like a land mine\" Times Literary Supplement', 'Fix-it Duck (Duck in the Truck): The trouble begins with a leaky roof in the sequel to Duck in a Truck, Fix-It Duck by Jez Alborough, and follows the, er, handy quacker on a series of missteps.Copyright 2002 Cahners Business Information, Inc.--This text refers to theHardcoveredition.']\n",
      "Resposta correta: True\n",
      "--------------------------------------------------\n",
      "Pergunta: Who is in a distant, timeless place?\n",
      "Resposta gerada: J.R. Tolkien\n",
      "Resposta esperada: The Prophet\n",
      "Contexto: [\"Ten Thousand Miles Without a Cloud: 'Packed with erudition and perception...it is also honest, sensitive, entirely without ego...ultimately, a meditation on the human condition.' Evening Standard 'Sun Shuyun records her feelings and those of others with spontaneous simplicity, almost innocence, as if she were still the child seeking her grandmother's solution.' Colin Thubron, Sunday Telegraph 'Wild Swans meets The Alchemist.' Conde Nast Traveller\", \"Out of the Silent Planet (Cosmic Trilogy): 'Adventure beyond our Earth - beautifully coloured and shaped.' The Times 'This book has real splendour, compelling moments and a flowing narrative.' New York Times\", \"History of Middle-Earth: Pt. 2: 'One marvels anew at the depth, breadth and persistence of J.R.R. Tolkien's labour. No one sympathetic to his aims - the invention of a secondary universe - will want to miss this chance to be present at the creation.' Publishers Weekly\"]\n",
      "Resposta correta: False\n",
      "--------------------------------------------------\n",
      "Pergunta: What is the most research-crammed fantasy?\n",
      "Resposta gerada: Autumn Story Brambly Hedge\n",
      "Resposta esperada: Autumn Story Brambly Hedge\n",
      "Contexto: ['Autumn Story Brambly Hedge: \"!the most research-crammed fantasy ever set before small children!\" Sunday Times Magazine', \"Flux: 'Arthur C. Clarke, Poul Anderson, Isaac Asimov and Robert Heinlein succeeded in doing it, but very few others. Now Stephen Baxter joins their exclusive ranks -- writing science fiction in which the science is right, the author knowledgeable, and the extrapolations a sheer pleasure to read, admire, enjoy. The reaction is that which C.S. Lewis referred to when he described science fiction as the only genuine consciousness-expanding drug. Flux is a highly imaginative and moving novel .. It is a rare thing to find such a good read. Wonderful stuff!' Harry Harrison, New Scientist  'Flux puts Stephen Baxter in the front line of world-spinners.' The Times\", 'Dangerous Pleasures: A Decade of Stories: \\'Nattily subversive, sexually ambiguous, intelligent and disturbing. The prose sizzles with acidic observation.\\' Sunday Times \\'Not one of these eleven stories is a dud. All of them are concerned with the fallout that occurs when soft-focussing fantasy collides with hard-nosed reality. The lingering after-effects \"lie on the sweeter side of bleak\". Witty, moving and very much alive.\\' Time Out \\'Gale has long been a master of short fiction. So it comes as no surprise to find that his first collection of stories shows him to be an adept of the art ... the form utilises all his strengths of acute observation, gentle wit and humane acceptance of human diversity ... Wit and wisdom, metaphor and moment constantly combine to delight.\\' The Times \\'Patrick Gale revels in absurd risks. It\\'s the promise of an unexpected, and potentially implausible outcome that entices you into his stories. The prose sizzles with acidic observations.\\' Independent on Sunday \\'Gale is a master of character, and he slips under the skins of his women protagonists with such wit that it\\'s often hard to believe he\\'s a man. From the misplaced passions of a jilted writer these fresh, clear-headed stories are reminiscent of Gale\\'s back catalogue of acclaimed novels.\\' Elle \\'Gale pins down the pain of love and leaving and the no-man\\'s-land between the apparently real and the illusory. He writes of uncertain memories and threatened loyalties and, in Dressing Up In Voices, of a couple whose passionate, inevitable break-up is traced with unrelenting accuracy.\\' Scotland on Sunday']\n",
      "Resposta correta: True\n",
      "--------------------------------------------------\n",
      "Pergunta: Complete the sentence: the form utilises all his strengths of acute observation\n",
      "Resposta gerada: Joel Lingeman\n",
      "Resposta esperada: gentle wit and humane acceptance of human diversity\n",
      "Contexto: ['Dumb Witness: Complete &amp; Unabridged: \"One of Poirot\\'s most brilliant achievements\" Glasgow Herald', \"Poirot: Omnibus: The Perfect Murders: 'Mrs Christie as usual puts a ring through the reader's nose and leads him to one of her smashing last-minute showdowns' Observer 'The empress of the crime novel' Daily Express 'Told briskly, vivaciously, and with ever-fertile imagination' Manchester Evening News--This text refers to an out of print or unavailable edition of this title.\", 'Unless: A Novel: \"A life is full of isolated events,\" writes Carol Shields near the end ofUnless, \"but these events, if they are to form a coherent narrative, require odd pieces of language to link them together, little chips of grammar (mostly adverbs or prepositions) that are hard to define... words liketherefore,else,other,also,thereof,therefore,instead,otherwise,despite,already, andnot yet.\" Shield\\'s explanation for her novel\\'s title lends meaning to this multilayered narrative in which a mother\\'s grief over a daughter\\'s break with the family revises her feminist outlook and pushes her craft as a writer in a new direction.The oldest daughter of 44-year-old Reta Winters suddenly, inexplicably, drops out of college and ends up on a Toronto street corner panhandling, with a cardboard sign around her neck that reads \"goodness.\" The quiet comforts of Reta\\'s small-town life and the constancy of her feminist perspective sustain her hope that her daughter will snap out of this, whatever \"this\" is. Threaded into her family\\'s crisis is her ongoing internal elegy on the exclusion of women from the literary canon, which she transposes to mean her daughter\\'s exclusion from humanity. Reta wonders if her daughter has discovered, as she herself did years before, that the world is \"an endless series of obstacles, an alignment of locked doors,\" and has chosen to pursue the one thing that doesn\\'t require power or a voice: goodness.In her own writing, Reta reaffirms her own sense of self, as well as her sense of humor. As her theoretical reflections on modern womanhood play counterpoint to her unwavering sense of creating a home and keeping her family together, Reta\\'s smarts and fears form a wonderfully coherent narrative--a life worth reading about. WithUnless, the inaugural title in HarperCollins\\'s Fourth Estate imprint, Shields (author of the Pulitzer Prize-winning novelThe Stone Diaries) once again asserts her place in the canon.--Emily Russin--This text refers to an out of print or unavailable edition of this title.']\n",
      "Resposta correta: False\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "file_path = './perguntas2.json'\n",
    "process_questions(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug R√°pido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextos relevantes: ['The Tao of Physics (Flamingo): \"A brilliant best seller. . . . Lucidly analyzes the tenets of Hinduism, Buddhism, and Taoism to show their striking parallels with the latest discovery in cyclotrons.\"--\"New York\" magazine \"A pioneering book of real value and wide appeal.\"--\"Washington Post\"\"I have been reading the book with amazement and the greatest interest, recommending it to everyone I meet and, as often as possible, in my lectures. I think you have done a magnificent and extremely important job.\"--\"Joseph Campbell\"', 'Encyclopedia of Essential Oils: The complete guide to the use of aromatic oils in aromatherapy, herbalism, health and well-being.: &#x201C;at last a clear and systematic distillation of useful information about a truly comprehensive spectrum of essential oils and absolutes.&#x201D; John Steele, American Aromatherapy Association.&#x201C;A comprehensive and timely contribution to aromatherapy, herbalism and the whole field of holistic health care. An authoritative, reliable guide that will serve its readers for many years.&#x201D; David Hoffmann, the American Herbalists Guild & California School of Herbal Studies.', \"The Element Encyclopedia of Witchcraft: The Complete A-Z for the Entire Magical World: Illes, the author of three previous books on magical spells, has compiled a lively and chatty reference on witches. The work is arranged topically, and there is a helpful index. Individual articles within the topic chapters sometimes haveseereferences as well. After a long introduction discussing several different and sometimes nearly mutually exclusive definitions of witches, chapters cover plants, animals, food and drink, magical beings, famous people in the magical world, the creative arts, witch persecution, and more. Chapters--or sections within the chapters--includeA-Zentries that range from a few paragraphs to a few pages. An extensive bibliography and a list of Internet resources complete the work.Illes focuses most strongly on Western European witchcraft practice and history, but she gives considerable attention to Asian, African, Mesoamerican, Caribbean, and South American practice as well. The creative arts chapter is up-to-date enough to discuss the Harry Potter phenomenon as well as many films, television programs, and other arts-related representations. Illes' take on the representation of witches inThe Wizard of Ozis in solidarity with witch practice.The book is long and could have used editing both to trim the text and to correct errors in spelling and grammar. Illes tends to overwrite when she feels passionately, and an editor would have helped with this as well. The horrors of witch persecutions--which continue in many places to this day--do not require overwriting, as William Burns shows very well inWitch Hunts in Europe and America(Greenwood, 2003), which is not in the bibliography.Academic libraries with extensive witchcraft collections may wish to consider this volume if only for the bibliography. Public libraries seeking a popular reference work on witchcraft could do much worse than this one. At the price, public libraries may wish to purchase some circulating copies as well.Kathleen StipekCopyright &#169; American Library Association. All rights reserved\"]\n",
      "Resposta: Encyclopedia of Essential Oils\n"
     ]
    }
   ],
   "source": [
    "# Debug\n",
    "question = \"What is the best encyclopedia of techonology?\" \n",
    "\n",
    "context = retriever.retrieve(question, k=3)\n",
    "print(\"Contextos relevantes:\", context)\n",
    "\n",
    "answer = generate_answer(question)\n",
    "print(f\"Resposta: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
